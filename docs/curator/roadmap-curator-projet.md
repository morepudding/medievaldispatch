# ğŸ—ï¸ Roadmap Projet Curator - Construction du pipeline IA

**Date** : 24 novembre 2025  
**Version** : 1.0  
**Statut** : Planification

---

## ğŸ“‹ Vue d'ensemble du projet

Le **Curator** est une application autonome locale qui gÃ©nÃ¨re automatiquement du contenu narratif et visuel pour Medieval Dispatch en utilisant l'IA.

### Objectifs
- âœ… GÃ©nÃ©rer textes (descriptions, dialogues, missions) via LLM local
- âœ… GÃ©nÃ©rer images (portraits, Ã©motions) via Stable Diffusion local
- âœ… SystÃ¨me de gÃ©nÃ©ration par niveaux (style â†’ types â†’ variations)
- âœ… Interface de contrÃ´le et monitoring
- âœ… DÃ©poser le contenu directement dans la DB Supabase

### Architecture technique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CURATOR APPLICATION                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Frontend        â”‚      â”‚  Backend         â”‚    â”‚
â”‚  â”‚  (Next.js 14)    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  (Python FastAPI)â”‚    â”‚
â”‚  â”‚                  â”‚ HTTP  â”‚                  â”‚    â”‚
â”‚  â”‚  - Dashboard     â”‚      â”‚  - Generators    â”‚    â”‚
â”‚  â”‚  - Controls      â”‚      â”‚  - Services      â”‚    â”‚
â”‚  â”‚  - Preview       â”‚      â”‚  - API Routes    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                       â”‚              â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                            â”‚  IA Services       â”‚   â”‚
â”‚                            â”‚                    â”‚   â”‚
â”‚                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚                            â”‚  â”‚ Ollama LLM   â”‚ â”‚   â”‚
â”‚                            â”‚  â”‚ (llama3:8b)  â”‚ â”‚   â”‚
â”‚                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚                            â”‚  â”‚ Stable Diff  â”‚ â”‚   â”‚
â”‚                            â”‚  â”‚ (WebUI API)  â”‚ â”‚   â”‚
â”‚                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ READ/WRITE
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Supabase DB       â”‚
                    â”‚   (PostgreSQL)      â”‚
                    â”‚                     â”‚
                    â”‚   Tables contenu:   â”‚
                    â”‚   - heroes          â”‚
                    â”‚   - missions        â”‚
                    â”‚   - dialogues       â”‚
                    â”‚   - buildings       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack technologique

**Frontend**
- Next.js 14 (App Router)
- TypeScript
- TailwindCSS
- Zustand (state management)

**Backend**
- Python 3.11+
- FastAPI
- psycopg2 (PostgreSQL)
- Ollama Python SDK
- Requests (Stable Diffusion API)

**IA Locale**
- Ollama (LLM) - Port 11434
- Stable Diffusion WebUI - Port 7860

**Infrastructure**
- Docker Compose
- PostgreSQL (Supabase distant)

---

## ğŸ¯ SPRINT 0 : Recherche technologique (2-3 jours)

**Objectif** : Valider la stack IA locale et mesurer les performances

### 0.1 - Installation environnement test

**PrÃ©requis matÃ©riel recommandÃ©** :
- CPU : 8 cores minimum
- RAM : 16 GB minimum (32 GB idÃ©al)
- GPU : NVIDIA RTX 3060 12GB minimum (pour Stable Diffusion)
- Stockage : 50 GB SSD libre

**Installation Ollama** :

```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# TÃ©lÃ©charger depuis https://ollama.com/download

# DÃ©marrer le service
ollama serve

# Tester avec modÃ¨le lÃ©ger
ollama pull llama3:8b
ollama run llama3:8b "Dis bonjour en franÃ§ais"
```

**Installation Stable Diffusion WebUI** :

```bash
# Clone repository
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
cd stable-diffusion-webui

# Lancer (installe automatiquement les dÃ©pendances)
./webui.sh --api --listen  # Linux/Mac
webui.bat --api --listen   # Windows

# TÃ©lÃ©charger modÃ¨le DreamShaper
wget https://civitai.com/api/download/models/128713 -O models/Stable-diffusion/dreamshaper_8.safetensors
```

---

### 0.2 - Tests de performance LLM

**Objectif** : Trouver le meilleur modÃ¨le vitesse/qualitÃ©

**ModÃ¨les Ã  tester** :

```bash
# Installer 3 modÃ¨les
ollama pull llama3:8b       # 4.7 GB - Rapide, polyvalent
ollama pull mistral:7b      # 4.1 GB - Bon franÃ§ais
ollama pull neural-chat:7b  # 4.1 GB - Dialogues naturels
```

**Script de test** : `tests/test_ollama.py`

```python
import ollama
import time

models = ['llama3:8b', 'mistral:7b', 'neural-chat:7b']

prompt = """
GÃ©nÃ¨re une description de hÃ©ros mÃ©diÃ©val-fantastique (200 mots).
Nom: Bjorn
Classe: Guerrier
Stats: Force 8, AgilitÃ© 4, Intelligence 3
Style: Direct, courageux, lÃ©gÃ¨rement bourru
"""

results = []

for model in models:
    print(f"\nğŸ§ª Test de {model}...")
    
    start = time.time()
    response = ollama.generate(
        model=model,
        prompt=prompt,
        options={
            'temperature': 0.7,
            'num_predict': 300
        }
    )
    duration = time.time() - start
    
    results.append({
        'model': model,
        'duration': duration,
        'text': response['response'],
        'tokens': response['eval_count']
    })
    
    print(f"â±ï¸ DurÃ©e: {duration:.2f}s")
    print(f"ğŸ“ Tokens: {response['eval_count']}")
    print(f"ğŸ“„ Extrait: {response['response'][:200]}...")

# RÃ©sultats
print("\n" + "="*60)
print("RÃ‰SUMÃ‰ PERFORMANCES")
print("="*60)
for r in results:
    print(f"{r['model']}: {r['duration']:.2f}s ({r['tokens']} tokens)")
```

**CritÃ¨res d'Ã©valuation** :
- âœ… Vitesse < 30s pour 200 mots
- âœ… QualitÃ© narrative (cohÃ©rence, style)
- âœ… Respect des contraintes (format, personnalitÃ©)

---

### 0.3 - Tests de performance Stable Diffusion

**Objectif** : Valider qualitÃ© et vitesse gÃ©nÃ©ration images

**ModÃ¨les Ã  tester** :
1. **DreamShaper 8** - Semi-rÃ©aliste Ã©quilibrÃ©
2. **Anything V5** - Style anime/manga
3. **ReV Animated** - Cartoon semi-rÃ©aliste

**Script de test** : `tests/test_stable_diffusion.py`

```python
import requests
import base64
import time
from pathlib import Path

SD_API_URL = "http://localhost:7860/sdapi/v1/txt2img"

test_prompts = [
    {
        "name": "warrior_bjorn",
        "prompt": "portrait of a medieval warrior, Bjorn, short beard, strong jaw, determined eyes, chainmail armor, fantasy art, detailed, semi-realistic",
        "negative": "blurry, low quality, distorted, ugly, modern, photo"
    },
    {
        "name": "mage_vi",
        "prompt": "portrait of a female mage, Vi, purple robes, mysterious eyes, magical aura, staff, fantasy art, detailed, semi-realistic",
        "negative": "blurry, low quality, distorted, ugly, modern, photo"
    }
]

for test in test_prompts:
    print(f"\nğŸ¨ Test de {test['name']}...")
    
    payload = {
        "prompt": test['prompt'],
        "negative_prompt": test['negative'],
        "steps": 30,
        "width": 512,
        "height": 512,
        "cfg_scale": 7,
        "sampler_name": "DPM++ 2M Karras",
        "seed": 42  # Reproductible
    }
    
    start = time.time()
    response = requests.post(SD_API_URL, json=payload, timeout=300)
    duration = time.time() - start
    
    if response.status_code == 200:
        image_data = base64.b64decode(response.json()['images'][0])
        
        output_path = Path(f"tests/output/{test['name']}.png")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_bytes(image_data)
        
        print(f"âœ… SuccÃ¨s en {duration:.2f}s")
        print(f"ğŸ’¾ SauvegardÃ©: {output_path}")
    else:
        print(f"âŒ Erreur: {response.status_code}")
```

**CritÃ¨res d'Ã©valuation** :
- âœ… Temps < 120s pour 512Ã—512px
- âœ… QualitÃ© visuelle (dÃ©tails, cohÃ©rence)
- âœ… ContrÃ´le du style via prompts
- âœ… ReproductibilitÃ© (seed fixe)

---

### 0.4 - Choix de la stack finale

**DÃ©cision basÃ©e sur les tests** :

```yaml
LLM_CHOICE: "llama3:8b"  # ou "mistral:7b" selon rÃ©sultats
SD_MODEL: "DreamShaper 8"  # Style semi-rÃ©aliste

CONFIGURATION:
  llm:
    temperature: 0.7
    max_tokens: 500
    num_predict: 500
    
  stable_diffusion:
    steps: 30
    cfg_scale: 7
    sampler: "DPM++ 2M Karras"
    width: 512
    height: 512
```

---

## ğŸ—ï¸ SPRINT 1 : Structure de base (1 semaine)

**Objectif** : CrÃ©er l'architecture curator avec frontend + backend

### 1.1 - Setup projet et Docker Compose

**Structure des dossiers** :

```
curator/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ database.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â””â”€â”€ app/
â””â”€â”€ tests/
    â”œâ”€â”€ test_ollama.py
    â””â”€â”€ test_stable_diffusion.py
```

**Fichier** : `curator/docker-compose.yml`

```yaml
version: '3.8'

services:
  # Backend Python FastAPI
  curator-backend:
    build: ./backend
    container_name: curator-backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - OLLAMA_HOST=http://host.docker.internal:11434
      - SD_API_URL=http://host.docker.internal:7860
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - curator-frontend

  # Frontend Next.js
  curator-frontend:
    build: ./frontend
    container_name: curator-frontend
    ports:
      - "3001:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000

networks:
  default:
    name: curator-network
```

**Fichier** : `curator/.env.example`

```env
# Database Supabase
DATABASE_URL=postgresql://user:pass@db.xxx.supabase.co:6543/postgres?pgbouncer=true
DIRECT_URL=postgresql://user:pass@db.xxx.supabase.co:5432/postgres

# IA Services (local)
OLLAMA_HOST=http://localhost:11434
SD_API_URL=http://localhost:7860

# Configuration
LLM_MODEL=llama3:8b
SD_MODEL=dreamshaper_8
TEMPERATURE=0.7
```

---

### 1.2 - Backend Python FastAPI

**Structure complÃ¨te** :

```
backend/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py              # Point d'entrÃ©e FastAPI
â”œâ”€â”€ config.py            # Configuration depuis .env
â”œâ”€â”€ database.py          # Connexion PostgreSQL
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hero.py          # Pydantic models
â”‚   â”œâ”€â”€ mission.py
â”‚   â””â”€â”€ dialogue.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_service.py   # Interface Ollama
â”‚   â”œâ”€â”€ image_service.py # Interface Stable Diffusion
â”‚   â””â”€â”€ db_service.py    # CRUD database
â”œâ”€â”€ generators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hero_generator.py
â”‚   â”œâ”€â”€ mission_generator.py
â”‚   â””â”€â”€ dialogue_generator.py
â””â”€â”€ routers/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ heroes.py        # Endpoints /api/heroes
    â”œâ”€â”€ missions.py      # Endpoints /api/missions
    â””â”€â”€ generation.py    # Endpoints /api/generate
```

**Fichier** : `backend/Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Installer dÃ©pendances systÃ¨me
RUN apt-get update && apt-get install -y \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Copier requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier code
COPY . .

# Exposer port
EXPOSE 8000

# Commande de dÃ©marrage
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

**Fichier** : `backend/requirements.txt`

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
psycopg2-binary==2.9.9
python-dotenv==1.0.0
ollama==0.1.0
requests==2.31.0
Pillow==10.1.0
```

**Fichier** : `backend/config.py`

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Database
    database_url: str
    direct_url: str
    
    # IA Services
    ollama_host: str = "http://localhost:11434"
    sd_api_url: str = "http://localhost:7860"
    
    # Models
    llm_model: str = "llama3:8b"
    sd_model: str = "dreamshaper_8"
    
    # Generation params
    temperature: float = 0.7
    max_tokens: int = 500
    
    class Config:
        env_file = ".env"

settings = Settings()
```

**Fichier** : `backend/main.py`

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from routers import heroes, missions, generation

app = FastAPI(
    title="Medieval Dispatch Curator API",
    version="1.0.0"
)

# CORS pour Next.js frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Routes
app.include_router(heroes.router, prefix="/api/heroes", tags=["heroes"])
app.include_router(missions.router, prefix="/api/missions", tags=["missions"])
app.include_router(generation.router, prefix="/api/generate", tags=["generation"])

@app.get("/")
def root():
    return {
        "message": "Medieval Dispatch Curator API",
        "version": "1.0.0"
    }

@app.get("/health")
def health():
    return {
        "status": "ok",
        "services": {
            "database": "connected",
            "ollama": "available",
            "stable_diffusion": "available"
        }
    }
```

**Fichier** : `backend/services/llm_service.py`

```python
import ollama
from typing import Dict, Any
from config import settings

class LLMService:
    def __init__(self):
        self.model = settings.llm_model
        self.temperature = settings.temperature
        self.max_tokens = settings.max_tokens
    
    def generate_text(
        self, 
        prompt: str, 
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """GÃ©nÃ¨re du texte via Ollama"""
        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            options={
                "temperature": temperature or self.temperature,
                "num_predict": max_tokens or self.max_tokens
            }
        )
        return response['response']
    
    def generate_with_template(
        self, 
        template: str, 
        variables: Dict[str, Any]
    ) -> str:
        """GÃ©nÃ¨re avec un template de prompt"""
        prompt = template.format(**variables)
        return self.generate_text(prompt)

# Instance globale
llm_service = LLMService()
```

**Fichier** : `backend/services/image_service.py`

```python
import requests
import base64
from pathlib import Path
from config import settings

class ImageService:
    def __init__(self):
        self.api_url = settings.sd_api_url
    
    def generate_image(
        self,
        prompt: str,
        negative_prompt: str = "blurry, low quality, distorted",
        width: int = 512,
        height: int = 512,
        steps: int = 30,
        seed: int = -1
    ) -> bytes:
        """GÃ©nÃ¨re une image via Stable Diffusion API"""
        payload = {
            "prompt": prompt,
            "negative_prompt": negative_prompt,
            "width": width,
            "height": height,
            "steps": steps,
            "cfg_scale": 7,
            "sampler_name": "DPM++ 2M Karras",
            "seed": seed
        }
        
        response = requests.post(
            f"{self.api_url}/sdapi/v1/txt2img",
            json=payload,
            timeout=300
        )
        response.raise_for_status()
        
        image_data = response.json()['images'][0]
        return base64.b64decode(image_data)
    
    def save_image(self, image_bytes: bytes, filepath: Path):
        """Sauvegarde l'image gÃ©nÃ©rÃ©e"""
        filepath.parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, 'wb') as f:
            f.write(image_bytes)

# Instance globale
image_service = ImageService()
```

**Fichier** : `backend/database.py`

```python
import psycopg2
from psycopg2.extras import RealDictCursor
from config import settings

class DatabaseService:
    def __init__(self):
        self.conn_params = settings.database_url
    
    def get_connection(self):
        """CrÃ©e une connexion Ã  la DB"""
        return psycopg2.connect(
            self.conn_params,
            cursor_factory=RealDictCursor
        )
    
    def execute_query(self, query: str, params: tuple = None):
        """ExÃ©cute une query SELECT"""
        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(query, params)
                return cursor.fetchall()
    
    def execute_update(self, query: str, params: tuple = None):
        """ExÃ©cute une query UPDATE/INSERT"""
        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(query, params)
                conn.commit()
                return cursor.rowcount

# Instance globale
db_service = DatabaseService()
```

---

### 1.3 - Frontend Next.js

**Structure** :

```
frontend/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ package.json
â”œâ”€â”€ next.config.js
â”œâ”€â”€ tailwind.config.js
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”œâ”€â”€ page.tsx          # Dashboard
â”‚   â”œâ”€â”€ heroes/
â”‚   â”‚   â””â”€â”€ page.tsx
â”‚   â”œâ”€â”€ missions/
â”‚   â”‚   â””â”€â”€ page.tsx
â”‚   â””â”€â”€ generate/
â”‚       â””â”€â”€ page.tsx
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Dashboard.tsx
â”‚   â”œâ”€â”€ GenerationPanel.tsx
â”‚   â”œâ”€â”€ ContentPreview.tsx
â”‚   â””â”€â”€ ProgressBar.tsx
â””â”€â”€ lib/
    â”œâ”€â”€ api.ts
    â””â”€â”€ types.ts
```

**Fichier** : `frontend/Dockerfile`

```dockerfile
FROM node:20-alpine

WORKDIR /app

# Copier package files
COPY package*.json ./

# Installer dÃ©pendances
RUN npm install

# Copier code
COPY . .

# Exposer port
EXPOSE 3000

# Commande dev
CMD ["npm", "run", "dev"]
```

**Fichier** : `frontend/package.json`

```json
{
  "name": "curator-frontend",
  "version": "1.0.0",
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start"
  },
  "dependencies": {
    "next": "14.0.4",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "zustand": "^4.4.7"
  },
  "devDependencies": {
    "@types/node": "^20.10.5",
    "@types/react": "^18.2.45",
    "autoprefixer": "^10.4.16",
    "postcss": "^8.4.32",
    "tailwindcss": "^3.4.0",
    "typescript": "^5.3.3"
  }
}
```

**Fichier** : `frontend/lib/api.ts`

```typescript
const API_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000'

export const curatorAPI = {
  // Health check
  async health() {
    const res = await fetch(`${API_URL}/health`)
    return res.json()
  },

  // RÃ©cupÃ©rer placeholders depuis DB
  async getPlaceholderHeroes() {
    const res = await fetch(`${API_URL}/api/heroes/placeholders`)
    return res.json()
  },

  // GÃ©nÃ©rer contenu pour un hÃ©ros
  async generateHeroContent(heroId: string) {
    const res = await fetch(`${API_URL}/api/generate/hero/${heroId}`, {
      method: 'POST'
    })
    return res.json()
  },

  // GÃ©nÃ©rer portrait
  async generateHeroPortrait(heroId: string, emotion: string) {
    const res = await fetch(`${API_URL}/api/generate/hero/${heroId}/portrait`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ emotion })
    })
    return res.json()
  },

  // Batch generation
  async generateAllHeroes() {
    const res = await fetch(`${API_URL}/api/generate/heroes/batch`, {
      method: 'POST'
    })
    return res.json()
  }
}
```

**Fichier** : `frontend/app/page.tsx`

```typescript
'use client'

import { useState, useEffect } from 'react'
import { curatorAPI } from '@/lib/api'

export default function Dashboard() {
  const [stats, setStats] = useState({
    heroes: { total: 5, enriched: 0 },
    missions: { total: 15, enriched: 0 },
    dialogues: { total: 12, enriched: 0 }
  })

  const [status, setStatus] = useState<'idle' | 'generating' | 'error'>('idle')

  useEffect(() => {
    // Charger stats depuis backend
    loadStats()
  }, [])

  const loadStats = async () => {
    try {
      const health = await curatorAPI.health()
      console.log('Backend status:', health)
      // TODO: Fetch real stats
    } catch (error) {
      console.error('Failed to connect to backend:', error)
      setStatus('error')
    }
  }

  const handleGenerateAll = async (type: 'heroes' | 'missions' | 'dialogues') => {
    setStatus('generating')
    try {
      if (type === 'heroes') {
        const result = await curatorAPI.generateAllHeroes()
        console.log('Generation result:', result)
      }
      setStatus('idle')
    } catch (error) {
      console.error('Generation error:', error)
      setStatus('error')
    }
  }

  return (
    <div className="min-h-screen bg-gray-900 text-white p-8">
      <h1 className="text-4xl font-bold mb-8">Medieval Dispatch Curator</h1>
      
      <div className="grid grid-cols-3 gap-6">
        {/* Carte HÃ©ros */}
        <div className="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 className="text-2xl mb-4">HÃ©ros</h2>
          <div className="text-5xl font-bold mb-2">
            {stats.heroes.enriched}/{stats.heroes.total}
          </div>
          <p className="text-gray-400 mb-4">Enrichis</p>
          <button 
            onClick={() => handleGenerateAll('heroes')}
            disabled={status === 'generating'}
            className="w-full bg-blue-600 hover:bg-blue-700 disabled:bg-gray-600 px-4 py-2 rounded transition"
          >
            {status === 'generating' ? 'GÃ©nÃ©ration...' : 'GÃ©nÃ©rer tout'}
          </button>
        </div>

        {/* Carte Missions */}
        <div className="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 className="text-2xl mb-4">Missions</h2>
          <div className="text-5xl font-bold mb-2">
            {stats.missions.enriched}/{stats.missions.total}
          </div>
          <p className="text-gray-400 mb-4">Enrichies</p>
          <button 
            onClick={() => handleGenerateAll('missions')}
            disabled={status === 'generating'}
            className="w-full bg-green-600 hover:bg-green-700 disabled:bg-gray-600 px-4 py-2 rounded transition"
          >
            {status === 'generating' ? 'GÃ©nÃ©ration...' : 'GÃ©nÃ©rer tout'}
          </button>
        </div>

        {/* Carte Dialogues */}
        <div className="bg-gray-800 p-6 rounded-lg border border-gray-700">
          <h2 className="text-2xl mb-4">Dialogues</h2>
          <div className="text-5xl font-bold mb-2">
            {stats.dialogues.enriched}/{stats.dialogues.total}
          </div>
          <p className="text-gray-400 mb-4">Enrichis</p>
          <button 
            onClick={() => handleGenerateAll('dialogues')}
            disabled={status === 'generating'}
            className="w-full bg-purple-600 hover:bg-purple-700 disabled:bg-gray-600 px-4 py-2 rounded transition"
          >
            {status === 'generating' ? 'GÃ©nÃ©ration...' : 'GÃ©nÃ©rer tout'}
          </button>
        </div>
      </div>

      {status === 'error' && (
        <div className="mt-6 bg-red-900 border border-red-700 text-red-200 p-4 rounded">
          âŒ Erreur de connexion au backend. VÃ©rifiez que les services sont dÃ©marrÃ©s.
        </div>
      )}
    </div>
  )
}
```

---

### 1.4 - Commandes de dÃ©marrage

**Fichier** : `curator/README.md`

```markdown
# Curator - Guide de dÃ©marrage

## PrÃ©requis

1. Docker et Docker Compose installÃ©s
2. Ollama installÃ© et en cours d'exÃ©cution (port 11434)
3. Stable Diffusion WebUI en cours d'exÃ©cution (port 7860)
4. AccÃ¨s Ã  la DB Supabase

## Installation

```bash
# 1. Copier .env
cp .env.example .env
# Ã‰diter .env avec les vraies credentials

# 2. DÃ©marrer les services
docker-compose up -d

# 3. VÃ©rifier les logs
docker-compose logs -f
```

## AccÃ¨s

- Frontend: http://localhost:3001
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

## ArrÃªt

```bash
docker-compose down
```
```

---

## ğŸ“‹ SPRINT 2-6 : Generators par niveaux (4 semaines)

**Note** : Les sprints 2-6 implÃ©mentent la logique de gÃ©nÃ©ration par niveaux pour chaque type de contenu (hÃ©ros, missions, dialogues, etc.)

**Structure dÃ©taillÃ©e dans un document sÃ©parÃ©** : `docs/roadmap-curator-generators.md`

### AperÃ§u des sprints

| Sprint | Contenu | DurÃ©e | Livrables |
|--------|---------|-------|-----------|
| Sprint 2 | HÃ©ros (Niveau 1-3) | 1 semaine | 5 hÃ©ros + 25 portraits |
| Sprint 3 | Missions (Niveau 1-3) | 1 semaine | 15 missions + variantes |
| Sprint 4 | Dialogues (Niveau 1-3) | 1 semaine | 12 dialogues complets |
| Sprint 5 | BÃ¢timents & Ambient | 3-4 jours | 5 bÃ¢timents + 32 textes |
| Sprint 6 | Mission climax | 3-4 jours | 1 mission interactive |

---

## ğŸ“‹ Checklist complÃ¨te

### Sprint 0 - Recherche (2-3 jours)
- [ ] Installer Ollama + tester 3 modÃ¨les LLM
- [ ] Installer Stable Diffusion + tester 3 modÃ¨les visuels
- [ ] Script `tests/test_ollama.py`
- [ ] Script `tests/test_stable_diffusion.py`
- [ ] Validation vitesse/qualitÃ©
- [ ] Choix stack finale documentÃ©

### Sprint 1 - Structure (1 semaine)
- [ ] CrÃ©er `docker-compose.yml`
- [ ] Backend FastAPI complet
  - [ ] `main.py` avec routes
  - [ ] `config.py` configuration
  - [ ] `database.py` connexion DB
  - [ ] `services/llm_service.py`
  - [ ] `services/image_service.py`
  - [ ] `services/db_service.py`
- [ ] Frontend Next.js
  - [ ] Dashboard principal
  - [ ] API client (`lib/api.ts`)
  - [ ] Composants de base
- [ ] Docker build et test local
- [ ] Connexion DB Supabase validÃ©e
- [ ] Health check `/health` fonctionnel

### Sprint 2-6 - Generators
- [ ] Voir document sÃ©parÃ© `roadmap-curator-generators.md`

---

## â±ï¸ Estimation globale

| Phase | DurÃ©e | Description |
|-------|-------|-------------|
| **Sprint 0** | 2-3 jours | Recherche et validation stack IA |
| **Sprint 1** | 5-7 jours | Architecture backend + frontend |
| **Sprint 2-6** | 20-28 jours | ImplÃ©mentation generators |
| **TOTAL** | **27-38 jours** | Pipeline complet opÃ©rationnel |

---

## ğŸš€ Commandes rapides

```bash
# DÃ©marrer Ollama (dans un terminal sÃ©parÃ©)
ollama serve

# DÃ©marrer Stable Diffusion (dans un terminal sÃ©parÃ©)
cd stable-diffusion-webui/
./webui.sh --api --listen

# DÃ©marrer Curator
cd curator/
docker-compose up -d

# Voir logs
docker-compose logs -f curator-backend

# ArrÃªter tout
docker-compose down
```

---

**DerniÃ¨re mise Ã  jour** : 24 novembre 2025  
**Version** : 1.0  
**Statut** : PrÃªt pour Sprint 0
